# -*- coding: utf-8 -*-
"""Git_Automatic vertebrea countour on XRAY.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-ayxDYdrutJ6no0Qc6Um_YiMNxFes8MS

# Automatic Vertebrae Contour in X-Ray dataset

**Dataset Information**:
- The dataset is made up of images and segmentated mask from two diffrent sources.
- Manually annotated around 60 datasets for this exercise.

## Take a look at the dataset
"""



"""**Mounting google drive**"""

from google.colab import drive
drive.mount('/content/drive/')

DATA_DIR = '/content/drive/MyDrive/NASA/CapStone/vertebra'

# Commented out IPython magic to ensure Python compatibility.
import numpy as np 
import tensorflow as tf
import pandas as pd
from tqdm import tqdm
import os
from cv2 import imread, createCLAHE 
import cv2
from glob import glob
# %matplotlib inline
import matplotlib.pyplot as plt

image_path = os.path.join(DATA_DIR, 'train')
mask_path = os.path.join(DATA_DIR, 'trainannot')

image_path_val = os.path.join(DATA_DIR, 'val')
mask_path_val = os.path.join(DATA_DIR, 'valannot')

training_files_image = os.listdir(image_path)
training_files_mask = os.listdir(mask_path)
testing_files_image = os.listdir(image_path_val)
testing_files_mask = os.listdir(mask_path_val)


def getData(X_shape, flag = "test"):
    im_array = []
    mask_array = []
    
    if flag == "test":
        for i in tqdm(testing_files_image): 
            im = cv2.resize(cv2.imread(os.path.join(image_path_val,i)),(X_shape,X_shape))[:,:,0]         
            im_array.append(im)
            
        for i in tqdm(testing_files_mask):          
            mask = cv2.resize(cv2.imread(os.path.join(mask_path_val,i)),(X_shape,X_shape))[:,:,2]   #2        
            mask_array.append(mask)

        return im_array,mask_array
    
    if flag == "train":
        for im in tqdm(training_files_mask):        
            mask = cv2.resize(cv2.imread(os.path.join(mask_path,im)),(X_shape,X_shape))[:,:,2]#2        
            mask_array.append(mask)

        for i in tqdm(training_files_image): 
            img = cv2.resize(cv2.imread(os.path.join(image_path,i)),(X_shape,X_shape))[:,:,0]
            im_array.append(img)

        return im_array,mask_array

def plotMask(X,y):
    sample = []
    
    for i in range(6):
        left = X[i]
        right = y[i]
        combined = np.hstack((left,right))
        sample.append(combined)
        
        
    for i in range(0,6,3):

        plt.figure(figsize=(25,10))
        
        plt.subplot(2,3,1+i)
        plt.imshow(sample[i])
        
        plt.subplot(2,3,2+i)
        plt.imshow(sample[i+1])
        
        
        plt.subplot(2,3,3+i)
        plt.imshow(sample[i+2])
        
        plt.show()

# Load training and testing data
dim = 256*2
X_train,y_train = getData(dim,flag="train")
X_test, y_test = getData(dim)

"""# Data Check

Load the data!
"""

print("training set")
plotMask(X_train,y_train)
print("testing set")
plotMask(X_test,y_test)

"""Here, i am combinig train and test folders"""

X_train = np.array(X_train).reshape(len(X_train),dim,dim,1)
y_train = np.array(y_train).reshape(len(y_train),dim,dim,1)
X_test = np.array(X_test).reshape(len(X_test),dim,dim,1)
y_test = np.array(y_test).reshape(len(y_test),dim,dim,1)
assert X_train.shape == y_train.shape
assert X_test.shape == y_test.shape
images1 = np.concatenate((X_train,X_test),axis=0)
mask1  = np.concatenate((y_train,y_test),axis=0)

"""Data Augmentation"""

import cv2

import albumentations as A

aug = A.Compose([
    A.OneOf([A.RandomCrop(width=512, height=512),
                 A.PadIfNeeded(min_height=512, min_width=512, p=0.5)],p=0.4),
    A.RandomBrightnessContrast(brightness_limit=0.25, contrast_limit=0.25,p=0.5),
    A.Compose([A.RandomScale(scale_limit=(-0.15, 0.15), p=1, interpolation=1),
                            A.PadIfNeeded(512, 512, border_mode=cv2.BORDER_CONSTANT), 
                            A.Resize(512, 512, cv2.INTER_NEAREST), ],p=0.5),
    A.ShiftScaleRotate (shift_limit=0.325, scale_limit=0.15, rotate_limit=15,border_mode=cv2.BORDER_CONSTANT, p=1),
    A.Rotate(15,p=0.5),
    A.Blur(blur_limit=1, p=0.5),
    A.Downscale(scale_min=0.15, scale_max=0.25,  always_apply=False, p=0.5),
    A.GaussNoise(var_limit=(0.05, 0.1), mean=0, per_channel=True, always_apply=False, p=0.5),
    A.HorizontalFlip(p=0.25),
])

x_train1s=np.copy(images1)
y_train1s=np.copy(mask1)
count=0
while(count<4):
  x_aug2=np.copy(x_train1s)
  y_aug2=np.copy(y_train1s)
  for i in range(len(x_train1s)):
    augmented=aug(image=x_train1s[i,:,:,:],mask=y_train1s[i,:,:,:])
    x_aug2[i,:,:,:]= augmented['image']
    y_aug2[i,:,:,:]= augmented['mask']
  x_trains=np.concatenate((images1,x_aug2))
  y_trains=np.concatenate((mask1,y_aug2))
  if count == 9:
    break
  count += 1

images=np.copy(x_trains)
mask=np.copy(y_trains)

from keras.models import *
from keras.layers import *
from keras.optimizers import *
from keras import backend as keras
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint, LearningRateScheduler


def dice_coef(y_true, y_pred):
    y_true_f = keras.flatten(y_true)
    y_pred_f = keras.flatten(y_pred)
    intersection = keras.sum(y_true_f * y_pred_f)
    return (2. * intersection + 1) / (keras.sum(y_true_f) + keras.sum(y_pred_f) + 1)

def dice_coef_loss(y_true, y_pred):
    return -dice_coef(y_true, y_pred)

def ARelu(x, alpha=0.9, beta=0.9):
    alpha = tf.clip_by_value(alpha, clip_value_min=0.01, clip_value_max=0.99)
    beta  = 1 + tf.math.sigmoid(beta)
    x = tf.cast(x, 'float32')
    return tf.nn.relu(x) * beta - tf.nn.relu(-x) * alpha

"""## Model network and callbacks

Made use of U-Net architecture. You can read about them [here](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/).

I have customised the layers according to my need
"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization,concatenate,Conv2DTranspose,Dropout
from tensorflow.keras.models import Model

#improved Unet
def UNET1 (input_shape=(512,512,1),last_activation='sigmoid'):
    inputs=Input(shape=input_shape)
    
    conv1 = Conv2D(32,(3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)
    d1=Dropout(0.1)(conv1)
    conv2 = Conv2D(32,(3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(d1)
    b=BatchNormalization()(conv2)
    
    pool1 = MaxPooling2D(pool_size=(2, 2))(b)
    conv3 = Conv2D(64,(3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)
    d2=Dropout(0.2)(conv3)
    conv4 = Conv2D(64,(3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(d2)
    b1=BatchNormalization()(conv4)
    
    pool2 = MaxPooling2D(pool_size=(2, 2))(b1)
    conv5 = Conv2D(128,(3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)
    d3=Dropout(0.3)(conv5)
    conv6 = Conv2D(128,(3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(d3)
    b2=BatchNormalization()(conv6)
    
    pool3 = MaxPooling2D(pool_size=(2, 2))(b2)
    conv7 = Conv2D(256,(3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)
    d4=Dropout(0.4)(conv7)
    conv8 = Conv2D(256,(3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(d4)
    b3=BatchNormalization()(conv8)
    
    pool4 = MaxPooling2D(pool_size=(2, 2))(b3)
    conv9 = Conv2D(512,(3,3),activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)
    d5=Dropout(0.5)(conv9)
    conv10 = Conv2D(512,(3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(d5)
    b4=BatchNormalization()(conv10)

    ##
    pool41 = MaxPooling2D(pool_size=(2, 2))(b4)
    conv91 = Conv2D(1024,(3,3),activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool41)
    d51=Dropout(0.5)(conv91)
    conv101 = Conv2D(1024,(3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(d51)
    b41=BatchNormalization()(conv101)

    conv111 = Conv2DTranspose(1024,(4,4), activation = ARelu, padding = 'same', strides=(2,2),kernel_initializer = 'he_normal')(b41)
    x1= concatenate([conv111,conv10])
    conv121 = Conv2D(512,(3,3), activation = ARelu, padding = 'same', kernel_initializer = 'he_normal')(x1)
    d61=Dropout(0.4)(conv121)
    conv131 = Conv2D(512,(3,3), activation = ARelu, padding = 'same', kernel_initializer = 'he_normal')(d61)
    b51=BatchNormalization()(conv131)
    ######
    
    
    conv11 = Conv2DTranspose(512,(4,4), activation = ARelu, padding = 'same', strides=(2,2),kernel_initializer = 'he_normal')(b51)#(b4)
    x= concatenate([conv11,conv8])
    conv12 = Conv2D(256,(3,3), activation = ARelu, padding = 'same', kernel_initializer = 'he_normal')(x)
    d6=Dropout(0.4)(conv12)
    conv13 = Conv2D(256,(3,3), activation = ARelu, padding = 'same', kernel_initializer = 'he_normal')(d6)
    b5=BatchNormalization()(conv13)
    
    
    conv14 = Conv2DTranspose(256,(4,4), activation = ARelu, padding = 'same', strides=(2,2),kernel_initializer = 'he_normal')(b5)
    x1=concatenate([conv14,conv6])
    conv15 = Conv2D(128,3, activation = ARelu, padding = 'same', kernel_initializer = 'he_normal')(x1)
    d7=Dropout(0.3)(conv15)
    conv16 = Conv2D(128,3, activation = ARelu, padding = 'same', kernel_initializer = 'he_normal')(d7)
    b6=BatchNormalization()(conv16)
    
    conv17 = Conv2DTranspose(128,(4,4), activation = ARelu, padding = 'same',strides=(2,2), kernel_initializer = 'he_normal')(b6)
    x2=concatenate([conv17,conv4])
    conv18 = Conv2D(64,(3,3), activation = ARelu, padding = 'same', kernel_initializer = 'he_normal')(x2)
    d8=Dropout(0.2)(conv18)
    conv19 = Conv2D(64,(3,3) ,activation =ARelu, padding = 'same', kernel_initializer = 'he_normal')(d8)
    b7=BatchNormalization()(conv19)
    
    conv20 = Conv2DTranspose(64,(4,4), activation = ARelu, padding = 'same',strides=(2,2), kernel_initializer = 'he_normal')(b7)
    x3=concatenate([conv20,conv2])
    conv21 = Conv2D(32,(3,3) ,activation = ARelu, padding = 'same', kernel_initializer = 'he_normal')(x3)
    d9=Dropout(0.1)(conv21)
    conv22 = Conv2D(32,(3,3), activation = ARelu, padding = 'same', kernel_initializer = 'he_normal')(d9)
    
    outputs = Conv2D(1,(1,1), activation = last_activation, padding = 'same', kernel_initializer = 'he_normal')(conv22)
    model2 = Model( inputs = inputs, outputs = outputs)
    
    return model2

"""#### Compile and train the Unet Model"""

model=UNET1(input_shape=(512,512,1),last_activation='sigmoid')#smk changed to 256
model.summary()
 
#tf.keras.utils.plot_model(model, show_shapes=True)
#tf.keras.utils.plot_model( model,to_file='model.png',show_shapes=False,show_dtype=False,show_layer_names=False,rankdir='LR',expand_nested=False,dpi=96,layer_range=None,show_layer_activations=False)

"""## Callbacks, Early Stopping and Reduced LR

"""

from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau
weight_path="{}_weights.best.hdf5".format('cxr_reg')

checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, 
                             save_best_only=True, mode='min', save_weights_only = True)

reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, #0.5
                                   patience=15, #3
                                   verbose=1, mode='min', epsilon=0.0001, cooldown=2, min_lr=0.000001)#min_lr=1e-6 #0.0001 is better
early = EarlyStopping(monitor="val_loss", 
                      mode="min", 
                      patience=25)#15 # probably needs to be more patient, but kaggle time is limited
callbacks_list = [checkpoint, early, reduceLROnPlat]

"""#### Model training!


"""

from IPython.display import clear_output
from keras.optimizers import Adam 
from sklearn.model_selection import train_test_split


model.compile(optimizer='adam', loss=dice_coef_loss, metrics=[dice_coef])


 
train_vol, validation_vol, train_seg, validation_seg = train_test_split((images-127.0)/127.0, 
                                                            (mask>127).astype(np.float32), 
                                                            test_size = 0.1,random_state = 2018)# to check on 127 smk

train_vol, test_vol, train_seg, test_seg = train_test_split(train_vol,train_seg, 
                                                            test_size = 0.15, 
                                                            random_state = 2018)

loss_history = model.fit(x = train_vol,
                       y = train_seg,
                         batch_size = 4,# smk changed to 8
                  epochs = 200,
                  validation_data =(test_vol,test_seg) ,
                  callbacks=callbacks_list)


#clear_output()

"""## Plot the metric and evaluate """

fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 5))
ax1.plot(loss_history.history['loss'], '-', label = 'Loss')
ax1.plot(loss_history.history['val_loss'], '-', label = 'Validation Loss')
ax1.legend()

ax2.plot(100*np.array(loss_history.history['dice_coef']), '-', 
         label = 'Accuracy')

ax2.plot(100*np.array(loss_history.history['val_dice_coef']), '-',
         label = 'Validation Accuracy')
ax2.legend()

"""## Test the model

Drawing contours on the mask obtained with cv2 library.
"""

from google.colab.patches import cv2_imshow
import cv2
import matplotlib.pyplot as plt

pred_candidates = np.random.randint(1,validation_vol.shape[0],10)
preds = model.predict(validation_vol)

plt.clf()
plt.figure(figsize=(25,20))


for i in range(0,12,4):
    print(i)
    print("Index Picked : ",pred_candidates)
    
    plt.subplot(4,4,i+1)  
    plt.imshow(np.squeeze(validation_vol[pred_candidates[i]]))
    plt.xlabel("Base Image")
    

    predict1 = cv2.resize(preds[pred_candidates[i]], (512,512), interpolation=cv2.INTER_LANCZOS4)
    mask11=np.uint8(predict1*255)#

    _, mask111 = cv2.threshold(mask11, thresh=255/2, maxval=255, type=cv2.THRESH_BINARY)
    cnts,hieararch=cv2.findContours(mask111,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)
    img11 = cv2.drawContours(validation_vol[pred_candidates[i]], cnts, -1, (255, 0, 0) , 2)
 
    plt.subplot(4,4,i+2)
    plt.imshow(np.squeeze(img11))
    plt.xlabel("Predicted Contour")

pred_candidates.data.release()
preds.data.release()